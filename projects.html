<!DOCTYPE html>
<html lang="en">
<head>
  <script>
    (function () {
      const storedTheme = localStorage.getItem("theme");
      const prefersDark = window.matchMedia("(prefers-color-scheme: dark)").matches;

      if (storedTheme === "dark" || (!storedTheme && prefersDark)) {
        document.documentElement.classList.add("dark");
      }
    })();
  </script>

  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Projects — Rylee Albrecht</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>

  <header>
    <h1>Projects</h1>
  
    <nav class="main-nav">
      <button id="theme-toggle" aria-label="Toggle dark mode" aria-pressed="false">
        <svg class="icon sun" xmlns="http://www.w3.org/2000/svg" fill="none"
          viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
          <path stroke-linecap="round" stroke-linejoin="round"
            d="M12 3v2.25m6.364.386-1.591 1.591M21 12h-2.25m-.386 6.364-1.591-1.591M12 
            18.75V21m-4.773-4.227-1.591 1.591M5.25 12H3m4.227-4.773L5.636 5.636M15.75 12a3.75 
            3.75 0 1 1-7.5 0 3.75 3.75 0 1 1 7.5 0Z" />
        </svg>
  
        <svg class="icon moon" xmlns="http://www.w3.org/2000/svg" fill="none"
          viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor">
          <path stroke-linecap="round" stroke-linejoin="round"
            d="M21.752 15.002A9.72 9.72 0 0 1 18 15.75c-5.385 0-9.75-4.365-9.75-9.75 
            0-1.33.266-2.597.748-3.752A9.753 9.753 0 0 0 3 11.25C3 16.635 7.365 21 
            12.75 21a9.753 9.753 0 0 0 9.002-5.998Z" />
        </svg>
      </button>
  
      <ul class="nav-links" id="nav-links">
        <li><a href="index.html">Home</a></li>
        <li><a href="courses.html">Courses</a></li>
        <li><a href="projects.html" class="active">Projects</a></li>
        <li><a href="research.html">Research</a></li>
        <li><a href="contact.html">Contact</a></li>
      </ul>
  
      <div class="nav-toggle" id="nav-toggle">&#9776;</div>
    </nav>
  </header>

  <div class="container">

    <section>
      <h2>Photography Agent <span class="project-year">(Fall 2025)</span></h2>
    
      <p>
        The Photography Agent is a collaborative semester project focused on building a photo
        organization and editing system powered by an agentic large language model (LLM).
        The LLM interprets natural language user requests and calls specialized
        computer vision tools for photo filtering and editing.
      </p>
    
      <h3>LLM Agent (My Contribution)</h3>
      <ul>
        <li>Base of <strong>Llama-3-8B-Instruct</strong></li>
        <li>Fine-tuned using <strong>LoRA</strong> on 1,985 curated examples</li>
        <li>Trained to:
          <ul>
            <li>Generate valid JSON tool-call outputs</li>
            <li>Select appropriate tools based on user intent</li>
            <li>Respond naturally when no tool invocation is required</li>
            <li>Explain available tools when requested</li>
            <li>Handle ambiguous or invalid requests appropriately</li>
          </ul>
        </li>
        <li>Integrated a custom system prompt to guide agent behavior during training and inference</li>
        <li>Evaluated using a held-out, shuffled test set of 218 examples</li>
        <li>Achieved <strong>88% overall response accuracy</strong> across tool selection and conversational tasks</li>
      </ul>
    
      <h3>System Tools (Team Contributions)</h3>
        <ul>
          <li><strong>Focus tool:</strong> return blurry or non-blurry images</li>
          <li><strong>Exposure tool:</strong> return properly exposed, underexposed, or overexposed images</li>
          <li><strong>Color tool:</strong> return images with warm, cool, or neutral color tones</li>
          <li><strong>Album filtering tool:</strong> return images that match a specified topic</li>
          <li><strong>Background blur tool:</strong> apply background blur to selected images</li>
          <li><strong>Object removal tool:</strong> remove people or vehicles from selected images</li>
        </ul>

      <h3>Links</h3>
        <ul>
          <li><a href="https://github.com/douglasglover3/photography-agent-backend" class="text-link">Backend repository</a></li>
          <li><a href="https://github.com/douglasglover3/photography-agent-frontend" class="text-link">Frontend repository</a></li>
          <li><a href="docs/Photography Agent Report.pdf" target="_blank" rel="noopener" class="text-link">Project Report (PDF)</a></li></li>
        </ul>
    
      <h3>Demo of Photography Agent</h3>
      <a href="videos/photo_agent_demo.mp4" class="text-link">Watch demo in new tab</a>
      <div class="video-container">
        <video width="800" controls>
          <source src="videos/photo_agent_demo.mp4" preload="metadata" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </section>
    

    <section>
      <h2>Diabetic Retinopathy Classification Preprocessing Evaluation <span class="project-year">(Spring 2024)</span></h2>
      <p>In this project, a partner and I investigated the impact of image preprocessing techniques on diabetic retinopathy (DR) severity classification using deep learning models. To discover
        if there is a preprocessing method that is best accross models.   
      </p>
      <ul>
        <li>Fine-tuned and evaluated ResNet-50, EfficientNet, and a hybrid DenseNet–Swin Transformer (DenseSwin) on a dataset created from the 
          <a href="https://www.kaggle.com/c/aptos2019-blindness-detection" class="text-link">APTOS 2019</a> and <a href="https://www.kaggle.com/competitions/diabetic-retinopathy-detection" class="text-link">EyePACS</a> 
          retinal fundus datasets.</li>
        <li>Compared five preprocessing pipelines, including CLAHE-based contrast enhancement, histogram equalization, Gaussian subtractive normalization, and a baseline. preprocessing</li>
      </ul>

      <h3>My Contributions</h3>
        <ul>
          <li>Implemented and fine-tuned EfficientNet and DenseNet-based (DenseSwin) models.</li>
          <li>Implemented retinal image preprocessing pipelines, including Gaussian Subtractive Normalization, CLAHE on the green channel with median filtering, and standard preprocessing.</li>
          <li>Performed quantitative evaluation and result visualization, including accuracy, precision, recall, and F1 score comparisons across models and preprocessing methods.</li>
        </ul>

      <h3>Results</h3>

      <div class="DRC-results">
        <div class="DRC-results-text">
          <ul>
            <li>Evaluated accuracy, precision, recall, and F1-scores of each model fine-tuned on data using each preprocessing method.</li>
            <li>Plot shows that preprocessing effectiveness is model-dependent, with CLAHE + Gaussian filtering improving ResNet-50 and EfficientNet performance, while the hybrid DenseSwin model performed best with standard preprocessing.</li>
            <li>This highlights the importance of architecture-aware preprocessing strategies for DR screening systems.</li>
          </ul>
        </div>

        <div class="DRC-image">
          <a href="images/DRC_plots.png" target="_blank">
            <img
              src="images/DRC_plots.png"
              width="400"
              alt="Plot of model performance across preprocessing methods">
          </a>
        </div>
      </div>

      <h3>Links</h3>
        <ul>
          <li><a href="https://github.com/rymayl33/Diabetic_Retinopathy_Preprocessing" class="text-link">Github Repository</a></li>
          <li><a href="docs/DR_processing_eval_report.pdf" target="_blank" rel="noopener" class="text-link">Project Report (PDF)</a></li>
        </ul>
    </section>

    <section>
      <h2>Cell Type & Cancer Classification <span class="project-year">(Fall 2024)</span></h2>
    
      <p>
        This project applies deep learning to automated cell-type and cancer multi-classification using the
        CellNet medical imaging dataset. Models classify images into 19 cell types and distinguish
        benign cells from multiple cancer subtypes, with the goal of improving speed and reliability
        in medical image analysis.
      </p>
      <ul>
        <li>Fine-tuned and evaluated 4 models(ResNet, EfficientNet, Multilayer Perceptron (MLP), and Swin transformer) using the
          <a href="https://www.kaggle.com/datasets/johncapocyan/cellnet-beta-version" class="text-link">CellNet</a> dataset</li>
        <li>Evaluated models using test accuracy, weighted F1-score, training computation time, and confusion matrices</li>
      </ul>
    
      <h3>Swin Transformer (Primary Contribution)</h3>
      <ul>
        <li>Implemented a hierarchical Swin Transformer for large-scale medical image classification</li>
        <li>Used shifted-window self-attention for efficient, scalable training</li>
        <li>Explored architectural trade-offs to reduce compute without sacrificing accuracy</li>
        <li>Reduced embedding dimension and attention heads to significantly improve runtime</li>
        <li>Performed grid search over learning rate, batch size, and MLP ratio</li>
        <li>Final configuration achieved:
          <ul>
            <li><strong>Test Accuracy:</strong> 97.6%</li>
            <li><strong>Weighted F1 Score:</strong> 0.976</li>
          </ul>
        </li>
        <a href="images/swin_transformer_accuracy.png" target="_blank">
          <img src="images/swin_transformer_accuracy.png"
               width="400"
               alt="Plot of SWIN transformer test and training accuracy">
        </a>
        
        <a href="images/swin_transformer_loss.png" target="_blank">
          <img src="images/swin_transformer_loss.png"
               width="400"
               alt="Plot of SWIN transformer test and training loss">
        </a>
        
        <a href="images/swin_transformer_cm.png" target="_blank">
          <img src="images/swin_transformer_cm.png"
               width="400"
               alt="SWIN transformer confusion matrix">
        </a>
        
        <li>Maintained stable training with no observed overfitting</li>
        <li>Demonstrated strong generalization across all 19 classes</li>
      </ul>
    
      <h3>Baseline Models (Team Comparison)</h3>
      <ul>
        <li><strong>ResNet50:</strong> Strong performance with 83.9% test accuracy and 0.826 weighted F1</li>
        <li><strong>EfficientNetB0:</strong> Moderate accuracy but poor weighted F1 due to class imbalance</li>
        <li><strong>Multilayer Perceptron:</strong> Performed poorly on large-scale multi-class image data</li>
      </ul>
      
      <details>
        <summary>View results for baseline models (ResNet, EfficientNet, MLP)</summary>
    
        <p>
          The following models were evaluated during development but underperformed
          relative to the Swin Transformer in terms of weighted F1 score and convergence.
        </p>

        <h4>EfficientNet</h4>
          <a href="images/efficientnet_accuracy_loss.png" target="_blank">
            <img src="images/efficientnet_accuracy_loss.png"
                alt="EfficientNet training and testing accuracy/loss"
                width="400">
          </a>
          <p>Data</p>
          <a href="images/efficientnet_cm.png" target="_blank">
            <img src="images/efficientnet_cm.png"
                alt="EfficientNet confusion matrix"
                width="400">
          </a>

          <h4>ResNet50</h4>
          <a href="images/resnet_accuracy.png" target="_blank">
            <img src="images/resnet_accuracy.png"
                alt="ResNet training and testing accuracy"
                width="400">
          </a>

          <a href="images/resnet_cm.png" target="_blank">
            <img src="images/resnet_cm.png"
                alt="ResNet confusion matrix"
                width="400">
          </a>

          <h4>Multilayer Perceptron</h4>
          <a href="images/MLP_accuracy.png" target="_blank">
            <img src="images/MLP_accuracy.png"
                alt="MLP training and testing accuracy"
                width="400">
          </a>

          <a href="images/MLP_cm.png" target="_blank">
            <img src="images/MLP_cm.png"
                alt="MLP confusion matrix"
                width="400">
          </a>

      </details>
      
    
      <h3>Results & Conclusions</h3>
      <ul>
        <li>Swin Transformer significantly outperformed all baseline CNN and MLP models</li>
        <li>Hierarchical attention proved highly effective for large, diverse medical image datasets</li>
        <li>Weighted F1 score confirmed robust performance across imbalanced cancer classes</li>
        <li>Results highlight Swin Transformers as a strong backbone for medical imaging tasks</li>
      </ul>

      <h3>Links</h3>
        <ul>
          <li><a href="https://github.com/rymayl33/Cell_Classification" class="text-link">Github Repository</a></li>
          <li><a href="docs/Machine Learning Project Final Report.pdf" target="_blank" rel="noopener" class="text-link">Project Report (PDF)</a></li></li>
        </ul>
    </section>

    <section>
      <h2>Captcha Recognition <span class="project-year">(Fall 2023)</span></h2></h2>
      <p>Deep learning project to train a model to recognize and solve text-based and image-based captchas. code found at ...</p>
      <ul>
        <li>Fine-tuned  using the
          <a href="https://www.kaggle.com/datasets/fournierp/captcha-version-2-images" class="text-link">CAPTCHA</a> dataset</li>
        <li>Evaluated models using test accuracy, weighted F1-score, training computation time, and confusion matrices</li>
      </ul>

      <h3>Results</h3>

      <h3>Links</h3>
        <ul>
          <li><a href="https://github.com/rymayl33/captcha_recognition" class="text-link">Github Repository</a></li>
          <li><a href="docs/captcha_report.pdf" target="_blank" rel="noopener" class="text-link">Project Report (PDF)</a></li></li>
        </ul>
    </section>

  </div>

  <footer>
    <div class="footer-content">
      <nav class="footer-nav">
        <a href="index.html">Home</a>
        <a href="courses.html">Courses</a>
        <a href="projects.html" class="active">Projects</a>
        <a href="research.html">Research</a>
        <a href="contact.html">Contact</a>
      </nav>
  
      <p>© 2025 Rylee Albrecht — All Rights Reserved</p>
    </div>
  </footer>
  <script src="javascript/nav.js"></script>
  <script src="javascript/theme.js"></script>
</body>
</html>
