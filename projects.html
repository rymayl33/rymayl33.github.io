<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Projects — Rylee Albrecht</title>
  <link rel="stylesheet" href="style.css" />
</head>

<body>

  <header>
    <h1>Projects</h1>
    <nav>
      <a href="index.html">Home</a>
      <a href="courses.html">Courses</a>
      <a href="projects.html" class="active">Projects</a>
      <a href="research.html">Research</a>
      <a href="contact.html">Contact</a>
    </nav>
  </header>

  <div class="container">

    <section>
      <h2>Photography Agent <span class="project-year">(Fall 2025)</span></h2>
    
      <p>
        The Photography Agent is a collaborative semester project focused on building a photo
        organization and editing system powered by an agentic large language model (LLM).
        The LLM interprets natural language user requests and calls specialized
        computer vision tools for photo filtering and editing.
      </p>
    
      <h3>LLM Agent (My Contribution)</h3>
      <ul>
        <li>Base of <strong>Llama-3-8B-Instruct</strong></li>
        <li>Fine-tuned using <strong>LoRA</strong> on 1,985 curated examples</li>
        <li>Trained to:
          <ul>
            <li>Generate valid JSON tool-call outputs</li>
            <li>Select appropriate tools based on user intent</li>
            <li>Respond naturally when no tool invocation is required</li>
            <li>Explain available tools when requested</li>
            <li>Handle ambiguous or invalid requests appropriately</li>
          </ul>
        </li>
        <li>Integrated a custom system prompt to guide agent behavior during training and inference</li>
        <li>Evaluated using a held-out, shuffled test set of 218 examples</li>
        <li>Achieved <strong>88% overall response accuracy</strong> across tool selection and conversational tasks</li>
      </ul>
    
      <h3>System Tools (Team Contributions)</h3>
        <ul>
          <li><strong>Focus tool:</strong> return blurry or non-blurry images</li>
          <li><strong>Exposure tool:</strong> return properly exposed, underexposed, or overexposed images</li>
          <li><strong>Color tool:</strong> return images with warm, cool, or neutral color tones</li>
          <li><strong>Album filtering tool:</strong> return images that match a specified topic</li>
          <li><strong>Background blur tool:</strong> apply background blur to selected images</li>
          <li><strong>Object removal tool:</strong> remove people or vehicles from selected images</li>
        </ul>

      <h3>GitHub</h3>
        <ul>
          <li><a href="https://github.com/douglasglover3/photography-agent-backend" class="text-link">Backend repository</a></li>
          <li><a href="https://github.com/douglasglover3/photography-agent-frontend" class="text-link">Frontend repository</a></li>
        </ul>
    
      <h3>Demo of Photography Agent</h3>
      <a href="videos/photo_agent_demo.mp4" class="text-link">Watch demo in new tab</a>
      <div class="video-container">
        <video width="800" controls>
          <source src="videos/photo_agent_demo.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </section>
    

    <section>
      <h2>Diabetic Retinopathy Classification Preprocessing Evaluation <span class="project-year">(Spring 2024)</span></h2>
      <p>In this project, a partner and I investigated the impact of image preprocessing techniques on diabetic retinopathy (DR) severity classification using deep learning models. To discover
        if there is a preprocessing method that is best accross models.   
      </p>
      <ul>
        <li>Fine-tuned and evaluated ResNet-50, EfficientNet, and a hybrid DenseNet–Swin Transformer (DenseSwin) on a dataset created from the 
          <a href="https://www.kaggle.com/c/aptos2019-blindness-detection" class="text-link">APTOS 2019</a> and <a href="https://www.kaggle.com/competitions/diabetic-retinopathy-detection" class="text-link">EyePACS</a> 
          retinal fundus datasets.</li>
        <li>Compared five preprocessing pipelines, including CLAHE-based contrast enhancement, histogram equalization, Gaussian subtractive normalization, and a baseline. preprocessing</li>
      </ul>

      <h3>Results</h3>

      <div class="DRC-results">
        <ul class="DRC-results-text">
          <li>Evaluated accuracy, precision, recall, and F1-scores of each model fine-tuned on data using each preprocessing method.</li>
          <li>Plot shows that preprocessing effectiveness is model-dependent, with CLAHE + Gaussian filtering improving ResNet-50 and EfficientNet performance, while the hybrid DenseSwin model performed best with standard preprocessing.</li>
          <li>This highlights the importance of architecture-aware preprocessing strategies for DR screening systems.</li>
        </ul>

        <a href="images/DRC_plots.png" target="_blank" class="DRC-plot">
          <img
            src="images/DRC_plots.png"
            width="400"
            alt="Plot of model performance across preprocessing methods"
            class="DRC-image"
          >
        </a>
      </div>

      <h3>My Contributions</h3>
        <ul>
          <li>Implemented and fine-tuned EfficientNet and DenseNet-based (DenseSwin) models.</li>
          <li>Implemented retinal image preprocessing pipelines, including Gaussian Subtractive Normalization, CLAHE on the green channel with median filtering, and standard preprocessing.</li>
          <li>Performed quantitative evaluation and result visualization, including accuracy, precision, recall, and F1 score comparisons across models and preprocessing methods.</li>
        </ul>

      <h3>Links</h3>
        <ul>
          <li><a href="https://github.com/rymayl33/Diabetic_Retinopathy_Preprocessing" class="text-link">Github Repository</a></li>
          <li><a href="docs/DR_processing_eval_report.pdf" target="_blank" rel="noopener" class="text-link">Project Report (PDF)</a></li>
        </ul>
    </section>

    <section>
      <h2>Cell Type & Cancer Classification (CellNet) <span class="project-year">(Fall 2024)</span></h2>
    
      <p>
        This project applies deep learning to automated cell-type and cancer classification using the
        CellNet medical imaging dataset. Models classify images into 19 cell types and distinguish
        benign cells from multiple cancer subtypes, with the goal of improving speed and reliability
        in medical image analysis.
      </p>
    
      <h3>My Role</h3>
      <ul>
        <li>Prepared and organized the CellNet dataset for large-scale training</li>
        <li>Implemented and optimized the Swin Transformer model</li>
        <li>Conducted hyperparameter tuning and large-scale training experiments</li>
        <li>Contributed to the technical report, README documentation, and presentation slides</li>
      </ul>
    
      <h3>Dataset</h3>
      <ul>
        <li>CellNet: First Official Beta Test Version of the CellNet Medical Image Database</li>
        <li>19 cell types across benign and multiple cancer classes</li>
        <li>Images resized to 128 × 128 × 3</li>
        <li>Train / Validation / Test split: 75% / 10% / 15%</li>
        <li>Over 122,000 total images</li>
      </ul>
    
      <h3>Evaluation Metrics</h3>
      <ul>
        <li>Test accuracy</li>
        <li>Weighted F1 score (class-imbalanced evaluation)</li>
        <li>Training computation time</li>
        <li>Confusion matrices for per-class error analysis</li>
      </ul>
    
      <h3>Swin Transformer (Primary Contribution)</h3>
      <ul>
        <li>Implemented a hierarchical Swin Transformer for large-scale medical image classification</li>
        <li>Used shifted-window self-attention for efficient, scalable training</li>
        <li>Explored architectural trade-offs to reduce compute without sacrificing accuracy</li>
        <li>Reduced embedding dimension and attention heads to significantly improve runtime</li>
        <li>Performed grid search over learning rate, batch size, and MLP ratio</li>
        <li>Final configuration achieved:
          <ul>
            <li><strong>Test Accuracy:</strong> 97.6%</li>
            <li><strong>Weighted F1 Score:</strong> 0.976</li>
          </ul>
        </li>
        <a href="images/swin_transformer_accuracy.png" target="_blank">
          <img src="images/swin_transformer_accuracy.png"
               width="400"
               alt="Plot of SWIN transformer test and training accuracy">
        </a>
        
        <a href="images/swin_transformer_loss.png" target="_blank">
          <img src="images/swin_transformer_loss.png"
               width="400"
               alt="Plot of SWIN transformer test and training loss">
        </a>
        
        <a href="images/swin_transformer_cm.png" target="_blank">
          <img src="images/swin_transformer_cm.png"
               width="400"
               alt="SWIN transformer confusion matrix">
        </a>
        
        <li>Maintained stable training with no observed overfitting</li>
        <li>Demonstrated strong generalization across all 19 classes</li>
      </ul>
    
      <h3>Baseline Models (Team Comparison)</h3>
      <ul>
        <li><strong>ResNet50:</strong> Strong performance with 83.9% test accuracy and 0.826 weighted F1</li>
        <li><strong>EfficientNetB0:</strong> Moderate accuracy but poor weighted F1 due to class imbalance</li>
        <li><strong>Multilayer Perceptron:</strong> Performed poorly on large-scale multi-class image data</li>
      </ul>
      
      <details>
        <summary>View results for baseline models (ResNet, EfficientNet, MLP)</summary>
    
        <p>
          The following models were evaluated during development but underperformed
          relative to the Swin Transformer in terms of weighted F1 score and convergence.
        </p>

        <h4>EfficientNet</h4>
          <a href="images/efficientnet_accuracy_loss.png" target="_blank">
            <img src="images/efficientnet_accuracy_loss.png"
                alt="EfficientNet training and testing accuracy/loss"
                width="400">
          </a>

          <a href="images/efficientnet_cm.png" target="_blank">
            <img src="images/efficientnet_cm.png"
                alt="EfficientNet confusion matrix"
                width="400">
          </a>

          <h4>ResNet50</h4>
          <a href="images/resnet_accuracy.png" target="_blank">
            <img src="images/resnet_accuracy.png"
                alt="ResNet training and testing accuracy"
                width="400">
          </a>

          <a href="images/resnet_cm.png" target="_blank">
            <img src="images/resnet_cm.png"
                alt="ResNet confusion matrix"
                width="400">
          </a>

          <h4>Multilayer Perceptron</h4>
          <a href="images/MLP_accuracy.png" target="_blank">
            <img src="images/MLP_accuracy.png"
                alt="MLP training and testing accuracy"
                width="400">
          </a>

          <a href="images/MLP_cm.png" target="_blank">
            <img src="images/MLP_cm.png"
                alt="MLP confusion matrix"
                width="400">
          </a>

      </details>
      
    
      <h3>Results & Conclusions</h3>
      <ul>
        <li>Swin Transformer significantly outperformed all baseline CNN and MLP models</li>
        <li>Hierarchical attention proved highly effective for large, diverse medical image datasets</li>
        <li>Weighted F1 score confirmed robust performance across imbalanced cancer classes</li>
        <li>Results highlight Swin Transformers as a strong backbone for medical imaging tasks</li>
      </ul>
    
      <h3>Future Work</h3>
      <ul>
        <li>Train with higher-resolution images to capture finer cellular detail</li>
        <li>Increase training epochs with improved computational resources</li>
        <li>Further dataset curation and class balancing</li>
        <li>Explore transfer learning and hybrid CNN–Transformer architectures</li>
      </ul>

      <h3>GHithub Link</h3>
      <a href="https://github.com/rymayl33/Cell_Classification" class="text-link">https://github.com/rymayl33/Cell_Classification</a>
    </section>

    <section>
      <h2>Captcha Recognition <span class="project-year">(Fall 2023)</span></h2></h2>
      <p>Deep learning project to train a model to recognize and solve text-based and image-based captchas. code found at <a href="https://github.com/rymayl33/captcha_recognition" class="text-link">https://github.com/rymayl33/captcha_recognition</a></p>
    </section>

  </div>

  <footer>
    <div class="footer-content">
      <nav class="footer-nav">
        <a href="index.html">Home</a>
        <a href="courses.html">Courses</a>
        <a href="projects.html" class="active">Projects</a>
        <a href="research.html">Research</a>
        <a href="contact.html">Contact</a>
      </nav>
  
      <p>© 2025 Rylee Albrecht — All Rights Reserved</p>
    </div>

</body>
</html>
